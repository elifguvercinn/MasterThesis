{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ehrdata and the physionet2012 challenge dataset\n",
    "\n",
    "This is a small notebook instructing how\n",
    "\n",
    "1. To install `ehrdata`\n",
    "2. Install the required ehrapy package \"development install\" from the development branch on which ehrapy starts to support time-series operations\n",
    "3. How `ehrdata` can download and prepare the Physionet2012 Challenge dataset out of the box\n",
    "4. How a small sequence model can be trained\n",
    "\n",
    "Resources\n",
    "\n",
    "- Physionet2012 Challenge Paper: https://ieeexplore.ieee.org/abstract/document/6420376\n",
    "- Data Link: https://physionet.org/content/challenge-2012/1.0.0/\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Install ehrdata\n",
    "\n",
    "Install a development version:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: ehrdata==0.0.2a1 in /opt/anaconda3/envs/goodmimic/lib/python3.12/site-packages (0.0.2a1)\n",
      "Requirement already satisfied: anndata in /opt/anaconda3/envs/goodmimic/lib/python3.12/site-packages (from ehrdata==0.0.2a1) (0.11.4)\n",
      "Requirement already satisfied: awkward in /opt/anaconda3/envs/goodmimic/lib/python3.12/site-packages (from ehrdata==0.0.2a1) (2.8.1)\n",
      "Requirement already satisfied: duckdb in /opt/anaconda3/envs/goodmimic/lib/python3.12/site-packages (from ehrdata==0.0.2a1) (1.2.1)\n",
      "Requirement already satisfied: filelock in /opt/anaconda3/envs/goodmimic/lib/python3.12/site-packages (from ehrdata==0.0.2a1) (3.18.0)\n",
      "Requirement already satisfied: lamin-utils in /opt/anaconda3/envs/goodmimic/lib/python3.12/site-packages (from ehrdata==0.0.2a1) (0.13.11)\n",
      "Requirement already satisfied: requests in /opt/anaconda3/envs/goodmimic/lib/python3.12/site-packages (from ehrdata==0.0.2a1) (2.32.3)\n",
      "Requirement already satisfied: rich in /opt/anaconda3/envs/goodmimic/lib/python3.12/site-packages (from ehrdata==0.0.2a1) (13.9.4)\n",
      "Requirement already satisfied: session-info in /opt/anaconda3/envs/goodmimic/lib/python3.12/site-packages (from ehrdata==0.0.2a1) (1.0.0)\n",
      "Requirement already satisfied: xarray in /opt/anaconda3/envs/goodmimic/lib/python3.12/site-packages (from ehrdata==0.0.2a1) (2025.3.0)\n",
      "Requirement already satisfied: array-api-compat!=1.5,>1.4 in /opt/anaconda3/envs/goodmimic/lib/python3.12/site-packages (from anndata->ehrdata==0.0.2a1) (1.11.2)\n",
      "Requirement already satisfied: h5py>=3.7 in /opt/anaconda3/envs/goodmimic/lib/python3.12/site-packages (from anndata->ehrdata==0.0.2a1) (3.13.0)\n",
      "Requirement already satisfied: natsort in /opt/anaconda3/envs/goodmimic/lib/python3.12/site-packages (from anndata->ehrdata==0.0.2a1) (8.4.0)\n",
      "Requirement already satisfied: numpy>=1.23 in /opt/anaconda3/envs/goodmimic/lib/python3.12/site-packages (from anndata->ehrdata==0.0.2a1) (1.26.4)\n",
      "Requirement already satisfied: packaging>=24.2 in /opt/anaconda3/envs/goodmimic/lib/python3.12/site-packages (from anndata->ehrdata==0.0.2a1) (24.2)\n",
      "Requirement already satisfied: pandas!=2.1.0rc0,!=2.1.2,>=1.4 in /opt/anaconda3/envs/goodmimic/lib/python3.12/site-packages (from anndata->ehrdata==0.0.2a1) (2.2.3)\n",
      "Requirement already satisfied: scipy>1.8 in /opt/anaconda3/envs/goodmimic/lib/python3.12/site-packages (from anndata->ehrdata==0.0.2a1) (1.15.2)\n",
      "Requirement already satisfied: awkward-cpp==45 in /opt/anaconda3/envs/goodmimic/lib/python3.12/site-packages (from awkward->ehrdata==0.0.2a1) (45)\n",
      "Requirement already satisfied: fsspec>=2022.11.0 in /opt/anaconda3/envs/goodmimic/lib/python3.12/site-packages (from awkward->ehrdata==0.0.2a1) (2025.3.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/anaconda3/envs/goodmimic/lib/python3.12/site-packages (from requests->ehrdata==0.0.2a1) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/anaconda3/envs/goodmimic/lib/python3.12/site-packages (from requests->ehrdata==0.0.2a1) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/anaconda3/envs/goodmimic/lib/python3.12/site-packages (from requests->ehrdata==0.0.2a1) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/anaconda3/envs/goodmimic/lib/python3.12/site-packages (from requests->ehrdata==0.0.2a1) (2025.1.31)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /opt/anaconda3/envs/goodmimic/lib/python3.12/site-packages (from rich->ehrdata==0.0.2a1) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /opt/anaconda3/envs/goodmimic/lib/python3.12/site-packages (from rich->ehrdata==0.0.2a1) (2.15.1)\n",
      "Requirement already satisfied: stdlib_list in /opt/anaconda3/envs/goodmimic/lib/python3.12/site-packages (from session-info->ehrdata==0.0.2a1) (0.11.1)\n",
      "Requirement already satisfied: mdurl~=0.1 in /opt/anaconda3/envs/goodmimic/lib/python3.12/site-packages (from markdown-it-py>=2.2.0->rich->ehrdata==0.0.2a1) (0.1.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/anaconda3/envs/goodmimic/lib/python3.12/site-packages (from pandas!=2.1.0rc0,!=2.1.2,>=1.4->anndata->ehrdata==0.0.2a1) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/anaconda3/envs/goodmimic/lib/python3.12/site-packages (from pandas!=2.1.0rc0,!=2.1.2,>=1.4->anndata->ehrdata==0.0.2a1) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /opt/anaconda3/envs/goodmimic/lib/python3.12/site-packages (from pandas!=2.1.0rc0,!=2.1.2,>=1.4->anndata->ehrdata==0.0.2a1) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in /opt/anaconda3/envs/goodmimic/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas!=2.1.0rc0,!=2.1.2,>=1.4->anndata->ehrdata==0.0.2a1) (1.17.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install \"ehrdata==0.0.2a1\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Install ehrapy\n",
    "The ehrapy package does not yet support time-series operations, as this is a feature we're developing.\n",
    "\n",
    "But the latest developments can be used by executing the line below. This fetches the ehrapy repository, switches to the time-series branch, and installs the ehrapy package from this branch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fatal: destination path 'ehrapy' already exists and is not an empty directory.\n",
      "Already on 'feature/time-series'\n",
      "Your branch is behind 'origin/feature/time-series' by 3 commits, and can be fast-forwarded.\n",
      "  (use \"git pull\" to update your local branch)\n",
      "Obtaining file:///Users/elifguvercin/ehrapy\n",
      "  Installing build dependencies ... \u001b[done\n",
      "\u001b[?25h  Checking if build backend supports build_editable ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build editable ... \u001b[?25ldone\n",
      "\u001b[?25h  Installing backend dependencies ... \u001b[?25done\n",
      "\u001b[?25h  Preparing editable metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: bottleneck in /opt/anaconda3/envs/goodmimic/lib/python3.12/site-packages (from ehrapy==0.12.0) (1.4.2)\n",
      "Requirement already satisfied: dowhy in /opt/anaconda3/envs/goodmimic/lib/python3.12/site-packages (from ehrapy==0.12.0) (0.12)\n",
      "Requirement already satisfied: ehrdata in /opt/anaconda3/envs/goodmimic/lib/python3.12/site-packages (from ehrapy==0.12.0) (0.0.2a1)\n",
      "Requirement already satisfied: fhiry in /opt/anaconda3/envs/goodmimic/lib/python3.12/site-packages (from ehrapy==0.12.0) (3.2.2)\n",
      "Requirement already satisfied: filelock in /opt/anaconda3/envs/goodmimic/lib/python3.12/site-packages (from ehrapy==0.12.0) (3.18.0)\n",
      "Requirement already satisfied: fknni>=1.2.0 in /opt/anaconda3/envs/goodmimic/lib/python3.12/site-packages (from ehrapy==0.12.0) (1.2.0)\n",
      "Requirement already satisfied: igraph in /opt/anaconda3/envs/goodmimic/lib/python3.12/site-packages (from ehrapy==0.12.0) (0.11.8)\n",
      "Requirement already satisfied: imbalanced-learn in /opt/anaconda3/envs/goodmimic/lib/python3.12/site-packages (from ehrapy==0.12.0) (0.13.0)\n",
      "Requirement already satisfied: lamin-utils in /opt/anaconda3/envs/goodmimic/lib/python3.12/site-packages (from ehrapy==0.12.0) (0.13.11)\n",
      "Requirement already satisfied: lifelines>=0.30.0 in /opt/anaconda3/envs/goodmimic/lib/python3.12/site-packages (from ehrapy==0.12.0) (0.30.0)\n",
      "Requirement already satisfied: miceforest in /opt/anaconda3/envs/goodmimic/lib/python3.12/site-packages (from ehrapy==0.12.0) (6.0.3)\n",
      "Requirement already satisfied: missingno in /opt/anaconda3/envs/goodmimic/lib/python3.12/site-packages (from ehrapy==0.12.0) (0.5.2)\n",
      "Requirement already satisfied: numba>=0.60.0 in /opt/anaconda3/envs/goodmimic/lib/python3.12/site-packages (from ehrapy==0.12.0) (0.61.0)\n",
      "Collecting numpy>=2.0.0 (from ehrapy==0.12.0)\n",
      "  Using cached numpy-2.2.4-cp312-cp312-macosx_14_0_x86_64.whl.metadata (62 kB)\n",
      "Requirement already satisfied: pyampute in /opt/anaconda3/envs/goodmimic/lib/python3.12/site-packages (from ehrapy==0.12.0) (0.0.3)\n",
      "Requirement already satisfied: python-dateutil in /opt/anaconda3/envs/goodmimic/lib/python3.12/site-packages (from ehrapy==0.12.0) (2.9.0.post0)\n",
      "Requirement already satisfied: requests in /opt/anaconda3/envs/goodmimic/lib/python3.12/site-packages (from ehrapy==0.12.0) (2.32.3)\n",
      "Requirement already satisfied: rich in /opt/anaconda3/envs/goodmimic/lib/python3.12/site-packages (from ehrapy==0.12.0) (13.9.4)\n",
      "Requirement already satisfied: scanpy in /opt/anaconda3/envs/goodmimic/lib/python3.12/site-packages (from ehrapy==0.12.0) (1.11.0)\n",
      "Requirement already satisfied: scikit-misc in /opt/anaconda3/envs/goodmimic/lib/python3.12/site-packages (from ehrapy==0.12.0) (0.5.1)\n",
      "Requirement already satisfied: session-info2 in /opt/anaconda3/envs/goodmimic/lib/python3.12/site-packages (from ehrapy==0.12.0) (0.1.2)\n",
      "Requirement already satisfied: tableone in /opt/anaconda3/envs/goodmimic/lib/python3.12/site-packages (from ehrapy==0.12.0) (0.9.1)\n",
      "Requirement already satisfied: thefuzz[speedup] in /opt/anaconda3/envs/goodmimic/lib/python3.12/site-packages (from ehrapy==0.12.0) (0.22.1)\n",
      "Requirement already satisfied: scikit-learn in /opt/anaconda3/envs/goodmimic/lib/python3.12/site-packages (from fknni>=1.2.0->ehrapy==0.12.0) (1.5.2)\n",
      "Requirement already satisfied: faiss-cpu in /opt/anaconda3/envs/goodmimic/lib/python3.12/site-packages (from fknni>=1.2.0->ehrapy==0.12.0) (1.10.0)\n",
      "Requirement already satisfied: pandas in /opt/anaconda3/envs/goodmimic/lib/python3.12/site-packages (from fknni>=1.2.0->ehrapy==0.12.0) (2.2.3)\n",
      "Requirement already satisfied: scipy>=1.7.0 in /opt/anaconda3/envs/goodmimic/lib/python3.12/site-packages (from lifelines>=0.30.0->ehrapy==0.12.0) (1.15.2)\n",
      "Requirement already satisfied: matplotlib>=3.0 in /opt/anaconda3/envs/goodmimic/lib/python3.12/site-packages (from lifelines>=0.30.0->ehrapy==0.12.0) (3.10.1)\n",
      "Requirement already satisfied: autograd>=1.5 in /opt/anaconda3/envs/goodmimic/lib/python3.12/site-packages (from lifelines>=0.30.0->ehrapy==0.12.0) (1.7.0)\n",
      "Requirement already satisfied: autograd-gamma>=0.3 in /opt/anaconda3/envs/goodmimic/lib/python3.12/site-packages (from lifelines>=0.30.0->ehrapy==0.12.0) (0.5.0)\n",
      "Requirement already satisfied: formulaic>=0.2.2 in /opt/anaconda3/envs/goodmimic/lib/python3.12/site-packages (from lifelines>=0.30.0->ehrapy==0.12.0) (1.1.1)\n",
      "Requirement already satisfied: llvmlite<0.45,>=0.44.0dev0 in /opt/anaconda3/envs/goodmimic/lib/python3.12/site-packages (from numba>=0.60.0->ehrapy==0.12.0) (0.44.0)\n",
      "  Using cached numpy-2.1.3-cp312-cp312-macosx_14_0_x86_64.whl.metadata (62 kB)\n",
      "Requirement already satisfied: causal-learn>=0.1.3.0 in /opt/anaconda3/envs/goodmimic/lib/python3.12/site-packages (from dowhy->ehrapy==0.12.0) (0.1.4.1)\n",
      "Requirement already satisfied: cvxpy>=1.2.2 in /opt/anaconda3/envs/goodmimic/lib/python3.12/site-packages (from dowhy->ehrapy==0.12.0) (1.6.4)\n",
      "Requirement already satisfied: cython<3.0 in /opt/anaconda3/envs/goodmimic/lib/python3.12/site-packages (from dowhy->ehrapy==0.12.0) (0.29.37)\n",
      "Requirement already satisfied: joblib>=1.1.0 in /opt/anaconda3/envs/goodmimic/lib/python3.12/site-packages (from dowhy->ehrapy==0.12.0) (1.4.2)\n",
      "Requirement already satisfied: networkx>=2.8.5 in /opt/anaconda3/envs/goodmimic/lib/python3.12/site-packages (from dowhy->ehrapy==0.12.0) (3.4.2)\n",
      "Requirement already satisfied: statsmodels>=0.13.5 in /opt/anaconda3/envs/goodmimic/lib/python3.12/site-packages (from dowhy->ehrapy==0.12.0) (0.14.4)\n",
      "Requirement already satisfied: sympy>=1.10.1 in /opt/anaconda3/envs/goodmimic/lib/python3.12/site-packages (from dowhy->ehrapy==0.12.0) (1.13.3)\n",
      "Requirement already satisfied: tqdm>=4.64.0 in /opt/anaconda3/envs/goodmimic/lib/python3.12/site-packages (from dowhy->ehrapy==0.12.0) (4.67.1)\n",
      "Requirement already satisfied: anndata in /opt/anaconda3/envs/goodmimic/lib/python3.12/site-packages (from ehrdata->ehrapy==0.12.0) (0.11.4)\n",
      "Requirement already satisfied: awkward in /opt/anaconda3/envs/goodmimic/lib/python3.12/site-packages (from ehrdata->ehrapy==0.12.0) (2.8.1)\n",
      "Requirement already satisfied: duckdb in /opt/anaconda3/envs/goodmimic/lib/python3.12/site-packages (from ehrdata->ehrapy==0.12.0) (1.2.1)\n",
      "Requirement already satisfied: session-info in /opt/anaconda3/envs/goodmimic/lib/python3.12/site-packages (from ehrdata->ehrapy==0.12.0) (1.0.0)\n",
      "Requirement already satisfied: xarray in /opt/anaconda3/envs/goodmimic/lib/python3.12/site-packages (from ehrdata->ehrapy==0.12.0) (2025.3.0)\n",
      "Requirement already satisfied: db-dtypes in /opt/anaconda3/envs/goodmimic/lib/python3.12/site-packages (from fhiry->ehrapy==0.12.0) (1.4.2)\n",
      "Requirement already satisfied: google-cloud-bigquery in /opt/anaconda3/envs/goodmimic/lib/python3.12/site-packages (from fhiry->ehrapy==0.12.0) (3.30.0)\n",
      "Requirement already satisfied: texttable>=1.6.2 in /opt/anaconda3/envs/goodmimic/lib/python3.12/site-packages (from igraph->ehrapy==0.12.0) (1.7.0)\n",
      "Requirement already satisfied: sklearn-compat<1,>=0.1 in /opt/anaconda3/envs/goodmimic/lib/python3.12/site-packages (from imbalanced-learn->ehrapy==0.12.0) (0.1.3)\n",
      "Requirement already satisfied: threadpoolctl<4,>=2.0.0 in /opt/anaconda3/envs/goodmimic/lib/python3.12/site-packages (from imbalanced-learn->ehrapy==0.12.0) (3.6.0)\n",
      "Requirement already satisfied: lightgbm>=4.1.0 in /opt/anaconda3/envs/goodmimic/lib/python3.12/site-packages (from miceforest->ehrapy==0.12.0) (4.6.0)\n",
      "Requirement already satisfied: pyarrow>=6.0.1 in /opt/anaconda3/envs/goodmimic/lib/python3.12/site-packages (from miceforest->ehrapy==0.12.0) (19.0.1)\n",
      "Requirement already satisfied: seaborn in /opt/anaconda3/envs/goodmimic/lib/python3.12/site-packages (from missingno->ehrapy==0.12.0) (0.13.2)\n",
      "Requirement already satisfied: six>=1.5 in /opt/anaconda3/envs/goodmimic/lib/python3.12/site-packages (from python-dateutil->ehrapy==0.12.0) (1.17.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/anaconda3/envs/goodmimic/lib/python3.12/site-packages (from requests->ehrapy==0.12.0) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/anaconda3/envs/goodmimic/lib/python3.12/site-packages (from requests->ehrapy==0.12.0) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/anaconda3/envs/goodmimic/lib/python3.12/site-packages (from requests->ehrapy==0.12.0) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/anaconda3/envs/goodmimic/lib/python3.12/site-packages (from requests->ehrapy==0.12.0) (2025.1.31)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /opt/anaconda3/envs/goodmimic/lib/python3.12/site-packages (from rich->ehrapy==0.12.0) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /opt/anaconda3/envs/goodmimic/lib/python3.12/site-packages (from rich->ehrapy==0.12.0) (2.15.1)\n",
      "Requirement already satisfied: h5py>=3.7 in /opt/anaconda3/envs/goodmimic/lib/python3.12/site-packages (from scanpy->ehrapy==0.12.0) (3.13.0)\n",
      "Requirement already satisfied: legacy-api-wrap>=1.4 in /opt/anaconda3/envs/goodmimic/lib/python3.12/site-packages (from scanpy->ehrapy==0.12.0) (1.4.1)\n",
      "Requirement already satisfied: natsort in /opt/anaconda3/envs/goodmimic/lib/python3.12/site-packages (from scanpy->ehrapy==0.12.0) (8.4.0)\n",
      "Requirement already satisfied: packaging>=21.3 in /opt/anaconda3/envs/goodmimic/lib/python3.12/site-packages (from scanpy->ehrapy==0.12.0) (24.2)\n",
      "Requirement already satisfied: patsy!=1.0.0 in /opt/anaconda3/envs/goodmimic/lib/python3.12/site-packages (from scanpy->ehrapy==0.12.0) (1.0.1)\n",
      "Requirement already satisfied: pynndescent>=0.5 in /opt/anaconda3/envs/goodmimic/lib/python3.12/site-packages (from scanpy->ehrapy==0.12.0) (0.5.13)\n",
      "Requirement already satisfied: typing-extensions in /opt/anaconda3/envs/goodmimic/lib/python3.12/site-packages (from scanpy->ehrapy==0.12.0) (4.13.0)\n",
      "Requirement already satisfied: umap-learn!=0.5.0,>=0.5 in /opt/anaconda3/envs/goodmimic/lib/python3.12/site-packages (from scanpy->ehrapy==0.12.0) (0.5.7)\n",
      "Requirement already satisfied: jinja2>=3.1.4 in /opt/anaconda3/envs/goodmimic/lib/python3.12/site-packages (from tableone->ehrapy==0.12.0) (3.1.6)\n",
      "Requirement already satisfied: openpyxl>=3.1.2 in /opt/anaconda3/envs/goodmimic/lib/python3.12/site-packages (from tableone->ehrapy==0.12.0) (3.1.5)\n",
      "Requirement already satisfied: tabulate>=0.9.0 in /opt/anaconda3/envs/goodmimic/lib/python3.12/site-packages (from tableone->ehrapy==0.12.0) (0.9.0)\n",
      "Requirement already satisfied: rapidfuzz<4.0.0,>=3.0.0 in /opt/anaconda3/envs/goodmimic/lib/python3.12/site-packages (from thefuzz[speedup]->ehrapy==0.12.0) (3.12.2)\n",
      "Requirement already satisfied: array-api-compat!=1.5,>1.4 in /opt/anaconda3/envs/goodmimic/lib/python3.12/site-packages (from anndata->ehrdata->ehrapy==0.12.0) (1.11.2)\n",
      "Requirement already satisfied: graphviz in /opt/anaconda3/envs/goodmimic/lib/python3.12/site-packages (from causal-learn>=0.1.3.0->dowhy->ehrapy==0.12.0) (0.20.3)\n",
      "Requirement already satisfied: pydot in /opt/anaconda3/envs/goodmimic/lib/python3.12/site-packages (from causal-learn>=0.1.3.0->dowhy->ehrapy==0.12.0) (3.0.4)\n",
      "Requirement already satisfied: momentchi2 in /opt/anaconda3/envs/goodmimic/lib/python3.12/site-packages (from causal-learn>=0.1.3.0->dowhy->ehrapy==0.12.0) (0.1.8)\n",
      "Requirement already satisfied: osqp>=0.6.2 in /opt/anaconda3/envs/goodmimic/lib/python3.12/site-packages (from cvxpy>=1.2.2->dowhy->ehrapy==0.12.0) (0.6.7.post3)\n",
      "Requirement already satisfied: clarabel>=0.5.0 in /opt/anaconda3/envs/goodmimic/lib/python3.12/site-packages (from cvxpy>=1.2.2->dowhy->ehrapy==0.12.0) (0.10.0)\n",
      "Requirement already satisfied: scs>=3.2.4.post1 in /opt/anaconda3/envs/goodmimic/lib/python3.12/site-packages (from cvxpy>=1.2.2->dowhy->ehrapy==0.12.0) (3.2.7.post2)\n",
      "Requirement already satisfied: interface-meta>=1.2.0 in /opt/anaconda3/envs/goodmimic/lib/python3.12/site-packages (from formulaic>=0.2.2->lifelines>=0.30.0->ehrapy==0.12.0) (1.3.0)\n",
      "Requirement already satisfied: wrapt>=1.0 in /opt/anaconda3/envs/goodmimic/lib/python3.12/site-packages (from formulaic>=0.2.2->lifelines>=0.30.0->ehrapy==0.12.0) (1.17.2)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/anaconda3/envs/goodmimic/lib/python3.12/site-packages (from jinja2>=3.1.4->tableone->ehrapy==0.12.0) (3.0.2)\n",
      "Requirement already satisfied: mdurl~=0.1 in /opt/anaconda3/envs/goodmimic/lib/python3.12/site-packages (from markdown-it-py>=2.2.0->rich->ehrapy==0.12.0) (0.1.2)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /opt/anaconda3/envs/goodmimic/lib/python3.12/site-packages (from matplotlib>=3.0->lifelines>=0.30.0->ehrapy==0.12.0) (1.3.1)\n",
      "Requirement already satisfied: cycler>=0.10 in /opt/anaconda3/envs/goodmimic/lib/python3.12/site-packages (from matplotlib>=3.0->lifelines>=0.30.0->ehrapy==0.12.0) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /opt/anaconda3/envs/goodmimic/lib/python3.12/site-packages (from matplotlib>=3.0->lifelines>=0.30.0->ehrapy==0.12.0) (4.56.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /opt/anaconda3/envs/goodmimic/lib/python3.12/site-packages (from matplotlib>=3.0->lifelines>=0.30.0->ehrapy==0.12.0) (1.4.8)\n",
      "Requirement already satisfied: pillow>=8 in /opt/anaconda3/envs/goodmimic/lib/python3.12/site-packages (from matplotlib>=3.0->lifelines>=0.30.0->ehrapy==0.12.0) (11.1.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /opt/anaconda3/envs/goodmimic/lib/python3.12/site-packages (from matplotlib>=3.0->lifelines>=0.30.0->ehrapy==0.12.0) (3.2.1)\n",
      "Requirement already satisfied: et-xmlfile in /opt/anaconda3/envs/goodmimic/lib/python3.12/site-packages (from openpyxl>=3.1.2->tableone->ehrapy==0.12.0) (2.0.0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/anaconda3/envs/goodmimic/lib/python3.12/site-packages (from pandas->fknni>=1.2.0->ehrapy==0.12.0) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /opt/anaconda3/envs/goodmimic/lib/python3.12/site-packages (from pandas->fknni>=1.2.0->ehrapy==0.12.0) (2025.2)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/anaconda3/envs/goodmimic/lib/python3.12/site-packages (from sympy>=1.10.1->dowhy->ehrapy==0.12.0) (1.3.0)\n",
      "Requirement already satisfied: awkward-cpp==45 in /opt/anaconda3/envs/goodmimic/lib/python3.12/site-packages (from awkward->ehrdata->ehrapy==0.12.0) (45)\n",
      "Requirement already satisfied: fsspec>=2022.11.0 in /opt/anaconda3/envs/goodmimic/lib/python3.12/site-packages (from awkward->ehrdata->ehrapy==0.12.0) (2025.3.1)\n",
      "Requirement already satisfied: google-api-core<3.0.0dev,>=2.11.1 in /opt/anaconda3/envs/goodmimic/lib/python3.12/site-packages (from google-api-core[grpc]<3.0.0dev,>=2.11.1->google-cloud-bigquery->fhiry->ehrapy==0.12.0) (2.24.2)\n",
      "Requirement already satisfied: google-auth<3.0.0dev,>=2.14.1 in /opt/anaconda3/envs/goodmimic/lib/python3.12/site-packages (from google-cloud-bigquery->fhiry->ehrapy==0.12.0) (2.38.0)\n",
      "Requirement already satisfied: google-cloud-core<3.0.0dev,>=2.4.1 in /opt/anaconda3/envs/goodmimic/lib/python3.12/site-packages (from google-cloud-bigquery->fhiry->ehrapy==0.12.0) (2.4.3)\n",
      "Requirement already satisfied: google-resumable-media<3.0dev,>=2.0.0 in /opt/anaconda3/envs/goodmimic/lib/python3.12/site-packages (from google-cloud-bigquery->fhiry->ehrapy==0.12.0) (2.7.2)\n",
      "Requirement already satisfied: stdlib_list in /opt/anaconda3/envs/goodmimic/lib/python3.12/site-packages (from session-info->ehrdata->ehrapy==0.12.0) (0.11.1)\n",
      "Requirement already satisfied: googleapis-common-protos<2.0.0,>=1.56.2 in /opt/anaconda3/envs/goodmimic/lib/python3.12/site-packages (from google-api-core<3.0.0dev,>=2.11.1->google-api-core[grpc]<3.0.0dev,>=2.11.1->google-cloud-bigquery->fhiry->ehrapy==0.12.0) (1.69.2)\n",
      "Requirement already satisfied: protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<7.0.0,>=3.19.5 in /opt/anaconda3/envs/goodmimic/lib/python3.12/site-packages (from google-api-core<3.0.0dev,>=2.11.1->google-api-core[grpc]<3.0.0dev,>=2.11.1->google-cloud-bigquery->fhiry->ehrapy==0.12.0) (5.29.4)\n",
      "Requirement already satisfied: proto-plus<2.0.0,>=1.22.3 in /opt/anaconda3/envs/goodmimic/lib/python3.12/site-packages (from google-api-core<3.0.0dev,>=2.11.1->google-api-core[grpc]<3.0.0dev,>=2.11.1->google-cloud-bigquery->fhiry->ehrapy==0.12.0) (1.26.1)\n",
      "Requirement already satisfied: grpcio<2.0dev,>=1.33.2 in /opt/anaconda3/envs/goodmimic/lib/python3.12/site-packages (from google-api-core[grpc]<3.0.0dev,>=2.11.1->google-cloud-bigquery->fhiry->ehrapy==0.12.0) (1.71.0)\n",
      "Requirement already satisfied: grpcio-status<2.0.dev0,>=1.33.2 in /opt/anaconda3/envs/goodmimic/lib/python3.12/site-packages (from google-api-core[grpc]<3.0.0dev,>=2.11.1->google-cloud-bigquery->fhiry->ehrapy==0.12.0) (1.71.0)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /opt/anaconda3/envs/goodmimic/lib/python3.12/site-packages (from google-auth<3.0.0dev,>=2.14.1->google-cloud-bigquery->fhiry->ehrapy==0.12.0) (5.5.2)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /opt/anaconda3/envs/goodmimic/lib/python3.12/site-packages (from google-auth<3.0.0dev,>=2.14.1->google-cloud-bigquery->fhiry->ehrapy==0.12.0) (0.4.1)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /opt/anaconda3/envs/goodmimic/lib/python3.12/site-packages (from google-auth<3.0.0dev,>=2.14.1->google-cloud-bigquery->fhiry->ehrapy==0.12.0) (4.9)\n",
      "Requirement already satisfied: google-crc32c<2.0dev,>=1.0 in /opt/anaconda3/envs/goodmimic/lib/python3.12/site-packages (from google-resumable-media<3.0dev,>=2.0.0->google-cloud-bigquery->fhiry->ehrapy==0.12.0) (1.7.0)\n",
      "Requirement already satisfied: qdldl in /opt/anaconda3/envs/goodmimic/lib/python3.12/site-packages (from osqp>=0.6.2->cvxpy>=1.2.2->dowhy->ehrapy==0.12.0) (0.1.7.post5)\n",
      "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /opt/anaconda3/envs/goodmimic/lib/python3.12/site-packages (from pyasn1-modules>=0.2.1->google-auth<3.0.0dev,>=2.14.1->google-cloud-bigquery->fhiry->ehrapy==0.12.0) (0.6.1)\n",
      "Using cached numpy-2.1.3-cp312-cp312-macosx_14_0_x86_64.whl (6.6 MB)\n",
      "Building wheels for collected packages: ehrapy\n",
      "  Building editable for ehrapy (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for ehrapy: filename=ehrapy-0.12.0-py3-none-any.whl size=10899 sha256=67b89d54741ac58d6e49df05fbd647e46c343c2d1100e3ab40bb48bec8e194fe\n",
      "  Stored in directory: /private/var/folders/dr/p8psy_yx3lvcqnxcy6v4fpt00000gn/T/pip-ephem-wheel-cache-hsc3r_nd/wheels/6b/a7/86/56ef40f69f8e76197fd706e0bf8c0533c6e65e5eef97ccc4e0\n",
      "Successfully built ehrapy\n",
      "Installing collected packages: numpy, ehrapy\n",
      "  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 1.26.4\n",
      "    Uninstalling numpy-1.26.4:\n",
      "      Successfully uninstalled numpy-1.26.4\n",
      "  Attempting uninstall: ehrapy\n",
      "    Found existing installation: ehrapy 0.12.0\n",
      "    Uninstalling ehrapy-0.12.0:\n",
      "      Successfully uninstalled ehrapy-0.12.0\n",
      "Successfully installed ehrapy-0.12.0 numpy-2.1.3\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/theislab/ehrapy.git\n",
    "!cd ehrapy && git fetch && git checkout feature/time-series && pip install -e ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**IMPORTANT** You need to restart your Jupyter kernel after running the ehrapy dev install; Only then will juptyer recognize the freshly installed ehrapy package.\n",
    "\n",
    "Once you restarted, you don't need to run the two \"installations\" above, but can continue directly below here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Prepare the Physionet2012 Challenge Dataset\n",
    "\n",
    "### 3.1 Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.1.3\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "print(np.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting numpy==1.26.4\n",
      "  Using cached numpy-1.26.4-cp312-cp312-macosx_10_9_x86_64.whl.metadata (61 kB)\n",
      "Using cached numpy-1.26.4-cp312-cp312-macosx_10_9_x86_64.whl (20.3 MB)\n",
      "Installing collected packages: numpy\n",
      "  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 2.1.3\n",
      "    Uninstalling numpy-2.1.3:\n",
      "      Successfully uninstalled numpy-2.1.3\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "ehrapy 0.12.0 requires numpy>=2.0.0, but you have numpy 1.26.4 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed numpy-1.26.4\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install \"numpy==1.26.4\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.26.4\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "print(np.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import ehrdata as ed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Data Loading\n",
    "The physionet2012 challenge dataset is one of the ready-to-use datasets of `ehrdata`. See the documentation for further information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/goodmimic/lib/python3.12/site-packages/anndata/_core/aligned_df.py:68: ImplicitModificationWarning: Transforming to str index.\n",
      "  warnings.warn(\"Transforming to str index.\", ImplicitModificationWarning)\n",
      "/opt/anaconda3/envs/goodmimic/lib/python3.12/site-packages/ehrdata/core/ehrdata.py:117: ImplicitModificationWarning: Setting element `.layers['r_layer']` of view, initializing view as actual.\n",
      "  self.layers[R_LAYER_KEY] = input\n"
     ]
    }
   ],
   "source": [
    "edata = ed.dt.physionet2012()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `edata` object contains the data in different fields. The most important ones are\n",
    "\n",
    "- `edata.r` is 3D n_obs x n_var x n_timesteps numpy array containing the time series data\n",
    "- `edata.obs` is a dataframe containing static information about the individuals\n",
    "- `edata.var` is a dataframe which can contain information on the features; here, its quite boring and just the names\n",
    "- `edata.t` is a dataframe which can contain information about the timeaxis; here, its quite boring and just the number of the hours\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "edata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "edata.r.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "edata.r.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "edata.obs['In-hospital_death'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "edata.var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/goodmimic/lib/python3.12/site-packages/ehrdata/core/ehrdata.py:117: ImplicitModificationWarning: Setting element `.layers['r_layer']` of view, initializing view as actual.\n",
      "  self.layers[R_LAYER_KEY] = input\n"
     ]
    }
   ],
   "source": [
    "# one drop and make it 36 features\n",
    "edata = edata[:, edata.var_names != \"MechVent\"] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((36, 1), (48, 1))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "edata.var.shape,edata.t.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Training a Sequence Model\n",
    "\n",
    "#### 4.1 Data Preprocessing\n",
    "\n",
    "This shows how the data can be split, prepared into a pytorch dataloader, and be used for a small pytorch model. The model itself is not competitive or \"good\", but is just a small example."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We split the data into train, validation, and test splits, using the \"set\" labels provided in the Physionet2012 challenge dataset. Any other split is also possible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/goodmimic/lib/python3.12/site-packages/ehrdata/core/ehrdata.py:117: ImplicitModificationWarning: Setting element `.layers['r_layer']` of view, initializing view as actual.\n",
      "  self.layers[R_LAYER_KEY] = input\n",
      "/opt/anaconda3/envs/goodmimic/lib/python3.12/site-packages/ehrdata/core/ehrdata.py:117: ImplicitModificationWarning: Setting element `.layers['r_layer']` of view, initializing view as actual.\n",
      "  self.layers[R_LAYER_KEY] = input\n",
      "/opt/anaconda3/envs/goodmimic/lib/python3.12/site-packages/ehrdata/core/ehrdata.py:117: ImplicitModificationWarning: Setting element `.layers['r_layer']` of view, initializing view as actual.\n",
      "  self.layers[R_LAYER_KEY] = input\n"
     ]
    }
   ],
   "source": [
    "edata_train = edata[edata.obs[\"set\"] == \"set-a\"]\n",
    "edata_val = edata[edata.obs[\"set\"] == \"set-b\"]\n",
    "edata_test = edata[edata.obs[\"set\"] == \"set-c\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "We perform a zero-mean and unit-variance normalization. For this, ehrapy provides some functionality:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ehrapy as ep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler3d = ep.ts.StandardScaler3D()\n",
    "\n",
    "scaler3d.fit_transform(edata_train)\n",
    "scaler3d.transform(edata_val)\n",
    "scaler3d.transform(edata_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To impute missing values, we now perform last observation carried forward imputation.\n",
    "There exist also more sophisticated methods for time-series imputation, e.g. [SAITS (Du et al., 2023)](https://doi.org/10.1016/j.eswa.2023.119619)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "locf = ep.ts.LOCFImputer()\n",
    "\n",
    "locf.fit_transform(edata_train)\n",
    "locf.transform(edata_val)\n",
    "locf.transform(edata_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, let us briefly explore the shape of the data: We have 37 numeric features, measured for 48 hours, and roughly 4'000 patients per split. The dataset is imbalanced, with ca 14% in-hospital death cases per split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "EHRData object with n_obs x n_var = 11988 x 36, and a timeseries of 48 steps.\n",
       "             shape of .X: (0, 0) \n",
       "             shape of .r: (11988, 36, 48) "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "edata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "EHRData object with n_obs x n_var = 3993 x 36, and a timeseries of 48 steps.\n",
       "             shape of .X: (0, 0) \n",
       "             shape of .r: (3993, 36, 48) "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "edata_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "EHRData object with n_obs x n_var = 3998 x 36, and a timeseries of 48 steps.\n",
       "             shape of .X: (0, 0) \n",
       "             shape of .r: (3998, 36, 48) "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "edata_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\n",
    "    f\"positive rate in edata_train: {edata_train.obs['In-hospital_death'].mean():.4f}, absolute count: {edata_train.obs['In-hospital_death'].sum()}\"\n",
    ")\n",
    "\n",
    "print(\n",
    "    f\"positive rate in edata_val: {edata_val.obs['In-hospital_death'].mean():.4f}, absolute count: {edata_val.obs['In-hospital_death'].sum()}\"\n",
    ")\n",
    "\n",
    "print(\n",
    "    f\"positive rate in edata_test: {edata_test.obs['In-hospital_death'].mean():.4f}, absolute count: {edata_test.obs['In-hospital_death'].sum()}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.2 Model preparation\n",
    "\n",
    "The below part is not very specific for `ehrdata` or `ehrapyz` anymore, but more about `torch`.\n",
    "\n",
    "It just is a tiny showcase for a sequence model.\n",
    "\n",
    "We build a small sequence model in pytorch. More elaborate model architectures have been proposed for this dataset. Here, for demonstration purposes and as to avoid complexity, we focus on a simple architecture.\n",
    "\n",
    "In a nutshell, the model has a GRU block as sequence model, and a classification head."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "from torch.nn import functional as F\n",
    "from torch.optim.lr_scheduler import _LRScheduler\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from torchinfo import summary\n",
    "from sklearn.metrics import roc_auc_score, precision_recall_curve, accuracy_score, f1_score, auc, confusion_matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Reorder edata.r based on the random 6x6 grid\n",
    "flat_order = [\n",
    "    'FiO2', 'HCO3', 'Bilirubin', 'SaO2', 'Urine', 'NIDiasABP',\n",
    "    'TroponinI', 'Platelets', 'Na', 'RespRate', 'TroponinT', 'GCS',\n",
    "    'Glucose', 'SysABP', 'DiasABP', 'NIMAP', 'Lactate', 'pH',\n",
    "    'Cholesterol', 'PaO2', 'ALP', 'Weight', 'HCT', 'Mg',\n",
    "    'AST', 'WBC', 'Temp', 'NISysABP', 'Albumin', 'PaCO2',\n",
    "    'BUN', 'HR', 'K', 'MAP', 'ALT', 'Creatinine'\n",
    "]\n",
    "\n",
    "# Get feature indices from edata.var\n",
    "indices = [edata.var.index.get_loc(f) for f in flat_order]\n",
    "\n",
    "# Reorder time series data in edata.r\n",
    "edata_train.r = edata_train.r[:, indices, :]\n",
    "edata_val.r = edata_val.r[:, indices, :]\n",
    "edata_test.r = edata_test.r[:, indices, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reshape_to_6x6_tensor(x):\n",
    "    # x shape: (n_samples, 36, 48)\n",
    "    x = x.transpose(0, 2, 1)  # → (n_samples, 48, 36)\n",
    "    x = x.reshape(x.shape[0], x.shape[1], 6, 6)  # → (n_samples, 48, 6, 6)\n",
    "    x = x[:, :, None, :, :]  # → (n_samples, 48, 1, 6, 6)\n",
    "    return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = edata_train.obs[\"In-hospital_death\"].values\n",
    "y_val = edata_val.obs[\"In-hospital_death\"].values\n",
    "y_test = edata_test.obs[\"In-hospital_death\"].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Convert to tensors\n",
    "trn_ds = TensorDataset(torch.tensor(x_train).float(), torch.tensor(y_train).long())\n",
    "val_ds = TensorDataset(torch.tensor(x_val).float(), torch.tensor(y_val).long())\n",
    "tst_ds = TensorDataset(torch.tensor(x_test).float(), torch.tensor(y_test).long())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = reshape_to_6x6_tensor(edata_train.r)\n",
    "x_val = reshape_to_6x6_tensor(edata_val.r)\n",
    "x_test = reshape_to_6x6_tensor(edata_test.r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/latex": [
       "$\\displaystyle \\left( 3997, \\  48, \\  1, \\  6, \\  6\\right)$"
      ],
      "text/plain": [
       "(3997, 48, 1, 6, 6)"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Create data loaders\n",
    "trn_dl = DataLoader(trn_ds, batch_size=256, shuffle=True)\n",
    "val_dl = DataLoader(val_ds, batch_size=256, shuffle=False)\n",
    "tst_dl = DataLoader(tst_ds, batch_size=256, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'class Classifier(nn.Module):\\n    def __init__(self, raw_ni, no, input_time_steps=48, drop=0.5):\\n        super().__init__()\\n\\n        # First and second conv layer\\n        self.conv_block = nn.Sequential(\\n            CostumConv1d(raw_ni, 64, kernel=3, stride=1, pad=0, drop=drop),\\n            CostumConv1d(64, 128, kernel=3, stride=1, pad=0, drop=drop),\\n            nn.MaxPool1d(kernel_size=2, stride=2)\\n        )\\n        self.flatten = Flatten()\\n\\n        # Dynamically calculate flattened size\\n        with torch.no_grad():\\n            dummy_input = torch.zeros(1, raw_ni, input_time_steps)\\n            dummy_output = self.conv_block(dummy_input)\\n            flat_dim = dummy_output.view(1, -1).shape[1]\\n\\n        self.fc = nn.Sequential(\\n            nn.Dropout(drop), nn.Linear(flat_dim, 64), nn.ReLU(inplace=True),\\n            nn.Dropout(drop), nn.Linear(64, 64), nn.ReLU(inplace=True),\\n            nn.Linear(64, no)\\n        )\\n\\n    def forward(self, t_raw):\\n        x = self.conv_block(t_raw)\\n        x = self.flatten(x)\\n        return self.fc(x)'"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class CostumConv3d(nn.Module):\n",
    "    def __init__(self, ni, no, kernel, stride, pad, drop=None, activ=lambda: nn.ReLU(inplace=True)):\n",
    "        super().__init__()\n",
    "        assert drop is None or (0.0 < drop < 1.0)\n",
    "        layers = [nn.Conv3d(ni, no, kernel, stride, pad)]\n",
    "        if activ:\n",
    "            layers.append(activ())\n",
    "        if drop is not None:\n",
    "            layers.append(nn.Dropout3d(drop))  # 3D Dropout for consistency\n",
    "        self.layers = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.layers(x)\n",
    "\n",
    "\n",
    "class Flatten(nn.Module):\n",
    "    def __init__(self, keep_batch_dim=True):\n",
    "        super().__init__()\n",
    "        self.keep_batch_dim = keep_batch_dim\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.keep_batch_dim:\n",
    "            return x.view(x.size(0), -1)\n",
    "        return x.view(-1)\n",
    "\n",
    "\n",
    "class Classifier(nn.Module):\n",
    "    def __init__(self, raw_ni=None, no=2, drop=0.5):\n",
    "        super().__init__()\n",
    "\n",
    "        self.conv_block = nn.Sequential(\n",
    "            CostumConv3d(1, 16, kernel=(3, 3, 3), stride=1, pad=1, drop=drop),\n",
    "            CostumConv3d(16, 32, kernel=(3, 3, 3), stride=1, pad=1, drop=drop),\n",
    "            nn.AdaptiveAvgPool3d((1, 1, 1))  # → (batch, 32, 1, 1, 1)\n",
    "        )\n",
    "\n",
    "        self.flatten = Flatten()\n",
    "\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Dropout(drop),\n",
    "            nn.Linear(32, 64),  # Output of conv block is (batch, 32)\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(drop),\n",
    "            nn.Linear(64, 64),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(64, no)\n",
    "        )\n",
    "\n",
    "    def forward(self, t_raw):\n",
    "        # t_raw shape: (batch, 1, 48, 6, 6)\n",
    "        x = self.conv_block(t_raw)  # → (batch, 32, 1, 1, 1)\n",
    "        x = self.flatten(x)         # → (batch, 32)\n",
    "        return self.fc(x)\n",
    "\n",
    "\n",
    "        \n",
    "\"\"\"class Classifier(nn.Module):\n",
    "    def __init__(self, raw_ni, no, input_time_steps=48, drop=0.5):\n",
    "        super().__init__()\n",
    "\n",
    "        # First and second conv layer\n",
    "        self.conv_block = nn.Sequential(\n",
    "            CostumConv1d(raw_ni, 64, kernel=3, stride=1, pad=0, drop=drop),\n",
    "            CostumConv1d(64, 128, kernel=3, stride=1, pad=0, drop=drop),\n",
    "            nn.MaxPool1d(kernel_size=2, stride=2)\n",
    "        )\n",
    "        self.flatten = Flatten()\n",
    "\n",
    "        # Dynamically calculate flattened size\n",
    "        with torch.no_grad():\n",
    "            dummy_input = torch.zeros(1, raw_ni, input_time_steps)\n",
    "            dummy_output = self.conv_block(dummy_input)\n",
    "            flat_dim = dummy_output.view(1, -1).shape[1]\n",
    "\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Dropout(drop), nn.Linear(flat_dim, 64), nn.ReLU(inplace=True),\n",
    "            nn.Dropout(drop), nn.Linear(64, 64), nn.ReLU(inplace=True),\n",
    "            nn.Linear(64, no)\n",
    "        )\n",
    "\n",
    "    def forward(self, t_raw):\n",
    "        x = self.conv_block(t_raw)\n",
    "        x = self.flatten(x)\n",
    "        return self.fc(x)\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([157, 48, 1, 6, 6]), torch.Size([157]))"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for epoch in range(1):\n",
    "    epoch_loss = 0\n",
    "    for i, batch in enumerate(trn_dl):\n",
    "        x_raw, y_batch = [t.to(device) for t in batch]\n",
    "x_raw.shape,y_batch.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([157, 1, 48, 6, 6])"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_raw=x_raw.permute(0, 2, 1, 3, 4)\n",
    "x_raw.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dummy input shape: torch.Size([1, 1, 48, 6, 6])\n",
      "Output shape: torch.Size([1, 2])\n",
      "Start model training\n"
     ]
    }
   ],
   "source": [
    "#raw_feat = x_train.shape[1]  # 37 features = 37 input channels\n",
    "num_classes = 2\n",
    "drop = 0.3\n",
    "loss_history = []\n",
    "acc_history = []\n",
    "patience, trials = 200, 0\n",
    "base, step = 1, 2\n",
    "best_acc = 0\n",
    "n_epochs = 100\n",
    "\n",
    "model = Classifier(no=num_classes, drop=drop).to(device)\n",
    "#criterion = nn.CrossEntropyLoss()\n",
    "criterion = nn.CrossEntropyLoss(weight=torch.tensor([1.0, 3.5]).to(device))#Compares predicted logits (before softmax) to true labels\n",
    "opt = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Example pass-through for shape check\n",
    "x_dummy = torch.randn(1,1,48,6,6).to(device)\n",
    "print(\"Dummy input shape:\", x_dummy.shape)\n",
    "with torch.no_grad():\n",
    "    print(\"Output shape:\", model(x_dummy).shape)\n",
    "\n",
    "model = Classifier(no=num_classes, drop=drop).to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "opt = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "print('Start model training')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Given groups=1, weight of size [16, 1, 3, 3, 3], expected input[256, 48, 1, 6, 6] to have 1 channels, but got 48 channels instead",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[68], line 7\u001b[0m\n\u001b[1;32m      5\u001b[0m x_raw, y_batch \u001b[38;5;241m=\u001b[39m x_raw\u001b[38;5;241m.\u001b[39mto(device), y_batch\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m      6\u001b[0m opt\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m----> 7\u001b[0m out \u001b[38;5;241m=\u001b[39m model(x_raw)\n\u001b[1;32m      8\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(out, y_batch)\n\u001b[1;32m      9\u001b[0m epoch_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n",
      "File \u001b[0;32m/opt/anaconda3/envs/goodmimic/lib/python3.12/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/goodmimic/lib/python3.12/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[64], line 51\u001b[0m, in \u001b[0;36mClassifier.forward\u001b[0;34m(self, t_raw)\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, t_raw):\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;66;03m# t_raw shape: (batch, 1, 48, 6, 6)\u001b[39;00m\n\u001b[0;32m---> 51\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv_block(t_raw)  \u001b[38;5;66;03m# → (batch, 32, 1, 1, 1)\u001b[39;00m\n\u001b[1;32m     52\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mflatten(x)         \u001b[38;5;66;03m# → (batch, 32)\u001b[39;00m\n\u001b[1;32m     53\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfc(x)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/goodmimic/lib/python3.12/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/goodmimic/lib/python3.12/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/goodmimic/lib/python3.12/site-packages/torch/nn/modules/container.py:217\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    215\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    216\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 217\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m module(\u001b[38;5;28minput\u001b[39m)\n\u001b[1;32m    218\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/goodmimic/lib/python3.12/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/goodmimic/lib/python3.12/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[64], line 13\u001b[0m, in \u001b[0;36mCostumConv3d.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m---> 13\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayers(x)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/goodmimic/lib/python3.12/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/goodmimic/lib/python3.12/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/goodmimic/lib/python3.12/site-packages/torch/nn/modules/container.py:217\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    215\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    216\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 217\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m module(\u001b[38;5;28minput\u001b[39m)\n\u001b[1;32m    218\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/goodmimic/lib/python3.12/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/goodmimic/lib/python3.12/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/goodmimic/lib/python3.12/site-packages/torch/nn/modules/conv.py:610\u001b[0m, in \u001b[0;36mConv3d.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    609\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 610\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_conv_forward(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweight, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/goodmimic/lib/python3.12/site-packages/torch/nn/modules/conv.py:605\u001b[0m, in \u001b[0;36mConv3d._conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    593\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mzeros\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    594\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mconv3d(\n\u001b[1;32m    595\u001b[0m         F\u001b[38;5;241m.\u001b[39mpad(\n\u001b[1;32m    596\u001b[0m             \u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    603\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroups,\n\u001b[1;32m    604\u001b[0m     )\n\u001b[0;32m--> 605\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mconv3d(\n\u001b[1;32m    606\u001b[0m     \u001b[38;5;28minput\u001b[39m, weight, bias, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstride, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdilation, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroups\n\u001b[1;32m    607\u001b[0m )\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Given groups=1, weight of size [16, 1, 3, 3, 3], expected input[256, 48, 1, 6, 6] to have 1 channels, but got 48 channels instead"
     ]
    }
   ],
   "source": [
    "for epoch in range(1, n_epochs + 1):\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    for x_raw, y_batch in trn_dl:\n",
    "        x_raw, y_batch = x_raw.to(device), y_batch.to(device)\n",
    "        opt.zero_grad()\n",
    "        out = model(x_raw)\n",
    "        loss = criterion(out, y_batch)\n",
    "        epoch_loss += loss.item()\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "\n",
    "    epoch_loss /= len(trn_dl)\n",
    "    loss_history.append(epoch_loss)\n",
    "\n",
    "    # Validation\n",
    "    model.eval()\n",
    "    correct, total = 0, 0\n",
    "    for x_raw, y_batch in val_dl:\n",
    "        x_raw, y_batch = x_raw.to(device), y_batch.to(device)\n",
    "        out = model(x_raw)\n",
    "        preds = F.log_softmax(out, dim=1).argmax(dim=1)\n",
    "        correct += (preds == y_batch).sum().item()\n",
    "        total += y_batch.size(0)\n",
    "\n",
    "    acc = correct / total\n",
    "    acc_history.append(acc)\n",
    "\n",
    "    if epoch % base == 0:\n",
    "        print(f\"Epoch: {epoch:3d}. Loss: {epoch_loss:.4f}. Acc.: {acc:2.2%}\")\n",
    "        base *= step\n",
    "\n",
    "    if acc > best_acc:\n",
    "        best_acc = acc\n",
    "        trials = 0\n",
    "        torch.save(model.state_dict(), \"best.pth\")\n",
    "        print(f\"Epoch {epoch} best model saved with accuracy: {best_acc:2.2%}\")\n",
    "    else:\n",
    "        trials += 1\n",
    "        if trials >= patience:\n",
    "            print(f\"Early stopping on epoch {epoch}\")\n",
    "            break\n",
    "\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def smooth(y, box_pts):\n",
    "    box = np.ones(box_pts)/box_pts\n",
    "    y_smooth = np.convolve(y, box, mode='same')\n",
    "    return y_smooth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f, ax = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "ax[0].plot(loss_history, label='loss')\n",
    "ax[0].set_title('Validation Loss History')\n",
    "ax[0].set_xlabel('Epoch no.')\n",
    "ax[0].set_ylabel('Loss')\n",
    "\n",
    "ax[1].plot(smooth(acc_history, 5)[:-2], label='acc')\n",
    "ax[1].set_title('Validation Accuracy History')\n",
    "ax[1].set_xlabel('Epoch no.')\n",
    "ax[1].set_ylabel('Accuracy');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_array = np.array([])\n",
    "for batch in tst_dl:\n",
    "        x_raw, y_batch = [t.to(device) for t in batch]\n",
    "        out = model(x_raw)\n",
    "        preds = F.log_softmax(out, dim=1).argmax(dim=1).numpy()\n",
    "        preds_array = np.concatenate((preds_array, preds), axis=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "confusion_matrix(y_test, preds_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "target_names = ['class 1', 'class 2']\n",
    "print(classification_report(y_test, preds_array, target_names=target_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arr_2d= np.array([[1,2,3], [5,6,7], [9,10,11], [12,13,14]])\n",
    "arr_2d.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arr_2d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arr_3d= arr_2d.reshape(2,2,3)\n",
    "arr_3d[:,:,0], arr_3d[:,:,1], arr_3d[:,:,2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arr_3d[:,:,0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary(model, input_size=(1, 37, 48)) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (goodmimic)",
   "language": "python",
   "name": "goodmimic"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
